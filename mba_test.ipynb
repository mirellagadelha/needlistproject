{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyM0UV4YludxaN00xfhaIaBV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mirellagadelha/needlistproject/blob/master/mba_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUnYSYU1AJ0p",
        "outputId": "6db7cf54-793e-4451-bcb4-5f1d7a038d8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: NVIDIA A100-SXM4-40GB\n",
            "Loading BERT tokenizer...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():\n",
        "\n",
        "    # Tell PyTorch to use the GPU.\n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from transformers import BertTokenizer\n",
        "import torch\n",
        "\n",
        "df = pd.read_csv(\"news-articles-with-type-short.csv\", delimiter=';', skiprows=1, names=['content', 'type'])\n",
        "\n",
        "contents = df.content.values\n",
        "types = df.type.values\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import Tensor\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "from typing import Any, Optional, Union\n",
        "\n",
        "from transformers import BatchEncoding\n",
        "\n",
        "def add_special_tokens_at_beginning_and_end(input_id_chunks: list[Tensor], mask_chunks: list[Tensor]) -> None:\n",
        "    \"\"\"\n",
        "    Adds special CLS token (token id = 101) at the beginning.\n",
        "    Adds SEP token (token id = 102) at the end of each chunk.\n",
        "    Adds corresponding attention masks equal to 1 (attention mask is boolean).\n",
        "    \"\"\"\n",
        "    for i in range(len(input_id_chunks)):\n",
        "        # adding CLS (token id 101) and SEP (token id 102) tokens\n",
        "        input_id_chunks[i] = torch.cat([Tensor([101]), input_id_chunks[i], Tensor([102])])\n",
        "        # adding attention masks  corresponding to special tokens\n",
        "        mask_chunks[i] = torch.cat([Tensor([1]), mask_chunks[i], Tensor([1])])\n",
        "\n",
        "def add_special_tokens_at_beginning_and_end(input_id_chunks, mask_chunks):\n",
        "    \"\"\"Add special tokens ([CLS] at the beginning, [SEP] at the end) for BERT input.\"\"\"\n",
        "    for i in range(len(input_id_chunks)):\n",
        "        # Convert input chunk and mask chunk to tensors if they are not already\n",
        "        input_chunk_tensor = torch.tensor(input_id_chunks[i])\n",
        "        mask_chunk_tensor = torch.tensor(mask_chunks[i])\n",
        "\n",
        "        # Add special tokens\n",
        "        input_id_chunks[i] = torch.cat([torch.tensor([101]), input_chunk_tensor, torch.tensor([102])])  # [CLS] and [SEP]\n",
        "        mask_chunks[i] = torch.cat([torch.tensor([1]), mask_chunk_tensor, torch.tensor([1])])  # Attention mask for [CLS] and [SEP]\n",
        "\n",
        "def add_padding_tokens(input_id_chunks: list[Tensor], mask_chunks: list[Tensor]) -> None:\n",
        "    \"\"\"Adds padding tokens (token id = 0) at the end to make sure that all chunks have exactly 512 tokens.\"\"\"\n",
        "    for i in range(len(input_id_chunks)):\n",
        "        # get required padding length\n",
        "        pad_len = 512 - input_id_chunks[i].shape[0]\n",
        "        # check if tensor length satisfies required chunk size\n",
        "        if pad_len > 0:\n",
        "            # if padding length is more than 0, we must add padding\n",
        "            input_id_chunks[i] = torch.cat([input_id_chunks[i], Tensor([0] * pad_len)])\n",
        "            mask_chunks[i] = torch.cat([mask_chunks[i], Tensor([0] * pad_len)])\n",
        "\n",
        "def tokenize_text_with_truncation(text: str, tokenizer, maximal_text_length: int):\n",
        "    \"\"\"Tokenizes text and truncates to the specified maximal length.\"\"\"\n",
        "    tokens = tokenizer.encode(text, add_special_tokens=True)\n",
        "    return tokens[:maximal_text_length]\n",
        "\n",
        "def tokenize_whole_text(text: str, tokenizer):\n",
        "    \"\"\"Tokenizes the entire text without truncation.\"\"\"\n",
        "    return tokenizer.encode(text, add_special_tokens=True)\n",
        "\n",
        "def stack_tokens_from_all_chunks(input_id_chunks: list[Tensor], mask_chunks: list[Tensor]) -> tuple[Tensor, Tensor]:\n",
        "    \"\"\"Reshapes data to a form compatible with BERT model input.\"\"\"\n",
        "    input_ids = torch.stack(input_id_chunks, dim=0)\n",
        "    attention_mask = torch.stack(mask_chunks, dim=0)\n",
        "\n",
        "    return input_ids.long(), attention_mask.int()\n",
        "\n",
        "def split_tokens_into_smaller_chunks(tokens, chunk_size: int, stride: int, minimal_chunk_length: int):\n",
        "    \"\"\"Splits tokens into smaller chunks.\"\"\"\n",
        "    input_id_chunks = []\n",
        "    mask_chunks = []\n",
        "\n",
        "    # Ensure we process the tokens in chunks with overlapping stride\n",
        "    for i in range(0, len(tokens), stride):\n",
        "        chunk = tokens[i:i + chunk_size]\n",
        "\n",
        "        # Only include chunks that are large enough\n",
        "        if len(chunk) >= minimal_chunk_length:\n",
        "            input_id_chunks.append(chunk)\n",
        "            mask_chunks.append([1] * len(chunk))  # Simple mask with 1's (can be adjusted based on padding)\n",
        "\n",
        "        if len(chunk) < chunk_size:\n",
        "            break\n",
        "\n",
        "    return input_id_chunks, mask_chunks"
      ],
      "metadata": {
        "id": "Ux7bUalcAkri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_single_text(\n",
        "    text: str,\n",
        "    tokenizer: tokenizer,\n",
        "    chunk_size: int,\n",
        "    stride: int,\n",
        "    minimal_chunk_length: int,\n",
        "    maximal_text_length: Optional[int],\n",
        ") -> tuple[Tensor, Tensor]:\n",
        "    \"\"\"Transforms (the entire) text to model input of BERT model.\"\"\"\n",
        "    if maximal_text_length:\n",
        "        tokens = tokenize_text_with_truncation(text, tokenizer, maximal_text_length)\n",
        "    else:\n",
        "        tokens = tokenize_whole_text(text, tokenizer)\n",
        "    input_id_chunks, mask_chunks = split_tokens_into_smaller_chunks(tokens, chunk_size, stride, minimal_chunk_length)\n",
        "    add_special_tokens_at_beginning_and_end(input_id_chunks, mask_chunks)\n",
        "    add_padding_tokens(input_id_chunks, mask_chunks)\n",
        "    input_ids, attention_mask = stack_tokens_from_all_chunks(input_id_chunks, mask_chunks)\n",
        "    return input_ids, attention_mask"
      ],
      "metadata": {
        "id": "5NC4EMPCAmBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BatchEncoding\n",
        "\n",
        "def transform_list_of_texts(\n",
        "    texts: list[str],\n",
        "    tokenizer: tokenizer,\n",
        "    chunk_size: int,\n",
        "    stride: int,\n",
        "    minimal_chunk_length: int,\n",
        "    maximal_text_length: Optional[int] = None,\n",
        ") -> BatchEncoding:\n",
        "    model_inputs = [\n",
        "        transform_single_text(text, tokenizer, chunk_size, stride, minimal_chunk_length, maximal_text_length)\n",
        "        for text in texts\n",
        "    ]\n",
        "    input_ids = [model_input[0] for model_input in model_inputs]\n",
        "    attention_mask = [model_input[1] for model_input in model_inputs]\n",
        "    tokens = {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n",
        "    return BatchEncoding(tokens)"
      ],
      "metadata": {
        "id": "tO-x2bHeAm6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenizedDataset(Dataset):\n",
        "    \"\"\"Dataset for tokens with optional labels.\"\"\"\n",
        "\n",
        "    def __init__(self, tokens: BatchEncoding, labels: Optional[list] = None):\n",
        "        self.input_ids = tokens[\"input_ids\"]\n",
        "        self.attention_mask = tokens[\"attention_mask\"]\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Union[tuple[Tensor, Tensor, Any], tuple[Tensor, Tensor]]:\n",
        "        if self.labels is not None and len(self.labels) > 0:\n",
        "            return self.input_ids[idx], self.attention_mask[idx], self.labels[idx]\n",
        "        return self.input_ids[idx], self.attention_mask[idx]"
      ],
      "metadata": {
        "id": "8LMamVhjAn3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = transform_list_of_texts(contents, tokenizer, 510, 510, 1, None)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTObHFZYAo3O",
        "outputId": "b25ca291-c302-4132-b9f9-4bf16f9b3f78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (723 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = TokenizedDataset(tokens, types)"
      ],
      "metadata": {
        "id": "8XKBSa-SAp9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size"
      ],
      "metadata": {
        "id": "lP3UuzMlAqys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('{:>5,} training samples'.format(train_size))\n",
        "print('{:>5,} validation samples'.format(val_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xdp3GqZov32",
        "outputId": "3ab3a52d-c704-458c-9b9a-cb8084137a7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  449 training samples\n",
            "   50 validation samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import Tensor\n",
        "\n",
        "def collate_fn_pooled_tokens(data):\n",
        "  input_ids = [data[i][0] for i in range(len(data))]\n",
        "  attention_mask = [data[i][1] for i in range(len(data))]\n",
        "  if len(data[0]) == 2:\n",
        "      collated = [input_ids, attention_mask]\n",
        "  else:\n",
        "      labels = Tensor([data[i][2] for i in range(len(data))])\n",
        "      collated = [input_ids, attention_mask, labels]\n",
        "  return collated"
      ],
      "metadata": {
        "id": "eqHNpEsnArxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler, random_split\n",
        "\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=32, collate_fn=collate_fn_pooled_tokens)\n",
        "validation_dataloader = DataLoader(val_dataset, sampler = SequentialSampler(val_dataset), batch_size = 32, collate_fn=collate_fn_pooled_tokens)"
      ],
      "metadata": {
        "id": "qGg1fie_Asi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels = [dataset.labels[i] for i in train_dataset.indices]\n",
        "val_labels = [dataset.labels[i] for i in val_dataset.indices]\n",
        "\n",
        "print(len(train_labels))  # Should be train_size\n",
        "print(len(val_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihehgF5fmwvF",
        "outputId": "98c24965-9589-4193-ce30-0961107ef650"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "449\n",
            "50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single\n",
        "# linear classification layer on top.\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
        "                    # You can increase this for multi-class tasks.\n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iqhj2Om5Atg5",
        "outputId": "49f250e0-09a0-4cbf-f38b-d270b96b087d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pooling_strategy = \"mean\"\n",
        "\n",
        "def _evaluate_single_batch(batch: tuple[Tensor]) -> Tensor:\n",
        "  input_ids = batch[0]\n",
        "  attention_mask = batch[1]\n",
        "  number_of_chunks = [len(x) for x in input_ids]\n",
        "\n",
        "  # concatenate all input_ids into one batch\n",
        "\n",
        "  input_ids_combined = []\n",
        "  for x in input_ids:\n",
        "      input_ids_combined.extend(x.tolist())\n",
        "\n",
        "  input_ids_combined_tensors = torch.stack([torch.tensor(x).to(device) for x in input_ids_combined])\n",
        "\n",
        "  # concatenate all attention masks into one batch\n",
        "\n",
        "  attention_mask_combined = []\n",
        "  for x in attention_mask:\n",
        "      attention_mask_combined.extend(x.tolist())\n",
        "\n",
        "  attention_mask_combined_tensors = torch.stack(\n",
        "      [torch.tensor(x).to(device) for x in attention_mask_combined]\n",
        "  )\n",
        "\n",
        "  # get model predictions for the combined batch\n",
        "  logits = model(input_ids_combined_tensors, attention_mask_combined_tensors).logits\n",
        "\n",
        "  logits_split = logits.split(number_of_chunks, dim=0)\n",
        "\n",
        "  # pooling\n",
        "  if pooling_strategy == \"mean\":\n",
        "      pooled_logits = torch.stack([torch.mean(x, dim=0) for x in logits_split])\n",
        "  elif pooling_strategy == \"max\":\n",
        "      pooled_logits = torch.stack([torch.max(x, dim=0)[0] for x in logits_split])\n",
        "  else:\n",
        "      raise ValueError(\"Unknown pooling strategy!\")\n",
        "\n",
        "  return pooled_logits"
      ],
      "metadata": {
        "id": "_R7A270kAvQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import AdamW, Optimizer\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "def _train_single_epoch(dataloader: DataLoader, optimizer: Optimizer) -> None:\n",
        "  model.train()\n",
        "  cross_entropy = CrossEntropyLoss()\n",
        "\n",
        "  for step, batch in enumerate(dataloader):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    labels = batch[-1].long().to(device)\n",
        "\n",
        "    # if isinstance(labels, list):\n",
        "    #   labels = torch.cat(labels, dim=0)\n",
        "\n",
        "    # # Ensure the labels are of type float32 for BCELoss\n",
        "    # if labels.dtype != torch.float32:\n",
        "    #   labels = labels.float()\n",
        "\n",
        "    logits = _evaluate_single_batch(batch)\n",
        "    loss = cross_entropy(logits, labels)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n"
      ],
      "metadata": {
        "id": "ka8H_oItAwf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = AdamW(model.parameters(), lr = 5e-05, eps = 1e-8)"
      ],
      "metadata": {
        "id": "_cGACd-CAxu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(3):\n",
        "  print('======== Epoch {:} / {:} ========'.format(epoch + 1, 3))\n",
        "  print('Training...')\n",
        "  _train_single_epoch(train_dataloader, optimizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrUkJyKwAyIt",
        "outputId": "9b1debeb-3b2e-47ed-bd7d-e234193c4195"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======== Epoch 1 / 3 ========\n",
            "Training...\n",
            "======== Epoch 2 / 3 ========\n",
            "Training...\n",
            "======== Epoch 3 / 3 ========\n",
            "Training...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(3):\n",
        "  print('======== Epoch {:} / {:} ========'.format(epoch + 1, 3))\n",
        "  print('Training...')\n",
        "  _train_single_epoch(train_dataloader, optimizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nbqyKmkPigN",
        "outputId": "ee454258-0604-4f45-d20e-5d480d07c38c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======== Epoch 1 / 3 ========\n",
            "Training...\n",
            "======== Epoch 2 / 3 ========\n",
            "Training...\n",
            "======== Epoch 3 / 3 ========\n",
            "Training...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from torch import argmax\n",
        "\n",
        "total_logits = []\n",
        "\n",
        "for step, batch in enumerate(validation_dataloader):\n",
        "  with torch.no_grad():\n",
        "    logits = _evaluate_single_batch(batch)\n",
        "    total_logits.append(logits)\n",
        "\n",
        "final_logits = torch.cat(total_logits, dim=0)\n",
        "classes = argmax(final_logits, dim=1)\n",
        "\n",
        "print(f\"Shape of predicted classes: {classes.shape}\")\n",
        "print(f\"Shape of val_labels: {len(val_labels)}\")\n",
        "\n",
        "# Calculate accuracy\n",
        "accurate = (classes == torch.tensor(val_labels)).sum().item()\n",
        "accuracy = accurate / len(val_labels)\n",
        "\n",
        "print(f\"Test accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "1oFTyNz5gRxI",
        "outputId": "109af1d3-2219-480b-9124-b3bdb670339f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of predicted classes: torch.Size([50])\n",
            "Shape of val_labels: 50\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-e892142927ae>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Calculate accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0maccurate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccurate\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
          ]
        }
      ]
    }
  ]
}